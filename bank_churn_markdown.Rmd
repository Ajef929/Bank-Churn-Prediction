---
title: "Bank Churn Prediction"
output:
  html_document:
    df_print: paged
---

Importing Libraries
```{r}
library(ggplot2)
library(dplyr)
library(GGally)
library(summarytools)
```

```{r}
##set seed for reproducability
set.seed(2022)
```

## The dataset

```{r}
df <- read.csv("Churn_Modelling.csv")
df <- sample_n(df,1000) #taking a sample

```

### Producing a dataset summary
```{r}
summary_df <- dfSummary(df)
summary_df
```

### Producing Descriptive Statistics

```{r}

descriptive_statistics <- summarytools::descr(df)
descriptive_statistics

```

### Data Preprocessing
```{r}
##adding a weight column to the data

group_df <- df %>% group_by(Exited) %>% summarise(groupCol = first(Exited),observationWeight = 1/(n()/nrow(df)))
df <- merge(df,group_df,on.x=GroupCol,on.y=Exited) %>% select(!c(groupCol))##join to original dataset and remove columns

##convert character columns to factors
df$Exited <- as.factor(df$Exited)
df$Gender <- as.factor(df$Gender)
df$Geography <- as.factor(df$Geography)


##remove surname and Customer ID as well as RowNumber
df <- df %>% select(!c(Surname,CustomerId,RowNumber))
str(df)
```

## Visualisation

### Correlations
```{r}
library(corrgram)
corrgram::corrgram(df, 
                   order = "HC", 
                   abs = TRUE, 
                   cor.method = "spearman",  ##spearman more robust to outliers
                   main = "Mroz correlation using grouping")

```
The only variables that are highly correlated are 'Balance' and 'NumOfProducts'. 'p_inv' can be ignored since it was cconstructed purely for the purposes of observation weighting.

## Pair plot
```{r}
selected_cols <-c("CreditScore","Tenure","Balance","EstimatedSalary","Age")
pairplot <- GGally::ggpairs(data=df,columns = selected_cols,aes(col=as.factor(Exited)))
pairplot
```
On visual inspection it is difficult to ascertain too much meaning from these plots. It should be noted that for almost all of these numeric variables there is some difference between customers who have left (blue) and customers that are retained (red). This is most obvious for the distributions of age in the far right-bottom corner.


### Checking variable continuity
Under normal data cleaning circumstances one should check the continuity of numeric variables. A sudden change may indicate an external effect to the data or missing information.

```{r}
cols <- c("CreditScore", "Balance")
d <- df[,cols]  # select the definitely-continuous columns
for (col in 1:ncol(d)) {
  d[,col] <- d[order(d[,col]),col] #sort each column in ascending order
}
d <- scale(x = d, center = TRUE, scale = TRUE)
mypalette <- rainbow(ncol(d))
matplot(x = seq(1, 100, length.out = nrow(d)), y = d, type = "l", xlab = "Percentile", ylab = "Values", lty = 1, lwd = 1, col = mypalette, main = "Rising value chart")
legend(legend = colnames(d), x = "topleft", y = "top", lty = 1, lwd = 1, col = mypalette, ncol = round(ncol(d)^0.3))
```

There do not appear to be any 'jumps' in values for creditscore, however, it is interesting to note that Balance has may zero values. This may affect the modelling process.
 

### Checking for homogeneity (constant variance)
```{r,fig.height=10,fig.width=10}
#cols <- c("CreditScore","Tenure","Balance","EstimatedSalary","Age","NumOfProducts") # choose the numeric columns
cols <- c("CreditScore","Age","Balance")
numData <- scale(df[,cols], center = TRUE, scale = TRUE)  ##scaled and centered
matplot(numData, type = "l", col = rainbow(ncol(numData)), xlab = "Observations in sequence", ylab = "Value") 
```
Variance over time is reasonably constant.


### boxplots
```{r,fig.width=10,fig.height=10}
library(car)
criterion <- 3
cols <- c("CreditScore","Tenure","Balance","EstimatedSalary","Age","NumOfProducts") # choose the numeric columns
numData <- scale(df[,cols], center = TRUE, scale = TRUE) 
car::Boxplot(numData, range = criterion, col = "red")

```
There are no univariable outliers in the sample.

### Mutlivariable outliers
```{r}
dummies <- model.matrix(~ . -observationWeight -1, df) # convert all variables to numeric but exclude the intercept
dumDF <- as.data.frame(dummies)
pca <- prcomp(formula = ~., data = dumDF, center = TRUE, scale. = TRUE)
ggplot() +
  geom_point(mapping = aes(x = pca$x[,1], y = pca$x[,2])) + # plot only the first two principle components
  labs(title = "Scatter plot of first two principle components", x = "PC1", y = "PC2")

```
Upon visual inspection the data has two main clusters for the first two principal components. There are no obvious multivariable outliers.

```{r}
##columns Exited+NumOfProducts+ Geography+Gender +HasCrCard +IsActiveMember
vcd::mosaic(formula = ~ NumOfProducts  + Geography + Gender, data = df, main = "Frequency novelties", legend = TRUE, shade = TRUE)

```
There are no initial overrepresentaions or underrepresentations of category instances found in combination of Geography, NumOfProducts and Gender.


## Experiment

A sample of 1000 datapoints were randomly selected from the dataset in the data preparation phase. Observation weighting was incorprated based on the prevalence of the response variable "Exited". Three methods were assessed and tuned using 10-fold cross validation, repeated 3 times. These methods were Logistic Regression, Support Vector Machines, and a Random Forest.

### Train-test split

Splitting the data using stratified random sampling. 
```{r}
Index <- caret::createDataPartition(y = df$Exited, p = 0.9, list = FALSE) ##stratefoed random split
train <- df[Index,]
test <- df[-Index,]
```


### Support Vector Machines
```{r}
library(caret)
#specify train control
tc <- trainControl(method = "repeatedcv",
                  repeats = 3,
                   number=10,
                   search="random",
                   )
##train model
mod <- caret::train(Exited ~ . -observationWeight, data = train, weights=train$observationWeight,method = "svmPoly",trControl=tc,verbose=TRUE,tuneLength=30)

```

```{r} 
print(mod)
```
#### evaluating the training process
```{r,fig.width=10,fig.height=10}
plot(mod,digits=2,plotType="scatter")##try level for a different type of plot

```
#### assessing performance on unseen data
```{r}
test_preds <- predict(mod, newdata = test)
cm <- confusionMatrix(test_preds, test$Exited)
cm
```

#### extracting the best fit
```{r}
best_params <- mod$bestTune
best_params
```

The best performing SVM model had a test accuracy of ___.


### Logistic Regression

```{r}
library(caret)
#specify train control
tc <- trainControl(method = "repeatedcv",
                  repeats = 3,
                   number=10,
                   search="random",
                   )

##train model
mod <- caret::train(Exited ~ . -observationWeight, data = train, weights=train$observationWeight,method = "glmnet",family="binomial",trControl=tc,verbose=TRUE,tuneLength=30)

```

```{r} 
print(mod)
```
#### evaluating the training process
```{r,fig.width=10,fig.height=10}
plot(mod,digits=2,plotType="scatter")##try level for a different type of plot
```


#### assessing performance on unseen data
```{r}
test_preds <- predict(mod, newdata = test)
cm <- confusionMatrix(test_preds, test$Exited)
cm
```


The best performing Logistic Regression reported an accuracy of ___.

### Random Forest
```{r}
#specify train control
tc <- trainControl(method = "repeatedcv",
                  repeats = 3,
                   number=10,
                   search="random",
                   )
##train model
mod <- caret::train(Exited ~ . -observationWeight, data = train, weights=train$observationWeight,method = "rf",trControl=tc,verbose=TRUE,tuneLength=30)

```

```{r} 
print(mod)
```

#### evaluating the training process
```{r,fig.width=10,fig.height=10}
plot(mod,digits=2,plotType="scatter") ##try level for a different type of plot
```

#### Assessing performance on unseen data
```{r}
test_preds <- predict(mod, newdata = test)
cm <- confusionMatrix(test_preds, test$Exited)
cm
```

### Discussion of results

Logistic regression performed most poorly with an accuracy of 76%. SVM and RF resulted in accuracies of 89% and 88% respectively. And so, Support Vector machines performed the greatest, especially when the data is the transformed using a polynomial kernel. It is possible that the data in question while not linearly separable is polynomially separable (with some relaxation of parameter C). 

### Best model further visualisation (SVM)

#### Fitting a new SVM model with the best parameter combinations. Plotting the decision boundary for 'Exited', visualised on the two most important variables.

```{r,fig.width=10,fig.height=10}
##support vector machine on the whole dataset

df <- read.csv("Churn_Modelling.csv")

###
group_df <- df %>% group_by(Exited) %>% summarise(groupCol = first(Exited),observationWeight = 1/(n()/nrow(df)))
df <- merge(df,group_df,on.x=GroupCol,on.y=Exited) %>% select(!c(groupCol))##join to original dataset and remove columns

##convert character columns to factors
df$Exited <- as.factor(df$Exited)
df$Gender <- as.factor(df$Gender)
df$Geography <- as.factor(df$Geography)

##remove surname and Customer ID as well as RowNumber
df <- df %>% select(!c(Surname,CustomerId,RowNumber))


##new train test split
Index <- caret::createDataPartition(y = df$Exited, p = 0.9, list = FALSE) ##stratefoed random split
train <- df[Index,]
test <- df[-Index,]



library(e1071)

##degree:2	scale:0.2497763	cost:0.3203732

fit <- svm(Exited ~ NumOfProducts + CreditScore + Geography + Gender + Age + Balance + Tenure + HasCrCard + IsActiveMember + EstimatedSalary, 
           data = train, 
           weight=train$observationWeight,
           scale = 0.249, 
           kernel = "polynomial",
           degree = 2,
           cost = 0.32)

test_preds <- predict(fit, newdata = test)
cm <- confusionMatrix(test_preds, test$Exited)
cm
```

```{r}
##creating an alluvial chart
library(alluvial)
data <- as.data.frame(cm$table)
data$Colour = ifelse(data$Prediction == data$Reference, "green", "red")
par(mar = c(0,0,1,0))
alluvial::alluvial(
  data[,c("Reference","Prediction")],
  freq = data$Freq,
  col = data$Colour,
  alpha = 0.5,
  hide = data$Freq == 0
)

#geomtext("Classification of Customer Churn", font = 2)
```

#### visualising variable importance

```{r}
##visualising support vector decision boundary fro 2 dimensions of data
##light blue corresponds to pointst that are actually Exited
plot(fit,data=test,NumOfProducts~Age,svSymbol = 1, dataSymbol = 2,symbolPalette = rainbow(2),fill=TRUE) ##plotting the decision boundary on test, 0 sybols are support vectors
```
When trained on a much larger sample of the dataset, Support Vector Machines performed with an accuracy of 85%. The model generalises reasonably well.  

## future investigation

In order to improve performance three main steps could be employed. Firstly, the models assessed should be trained on a much larger samples in order to provide more robust results on the test data. With the current data sample, simply changing the random selection of points may significantly impact performance metrics. Secondly, investigating the effect of feature engineering may provide better results. It would be interesting to see if there is any metadata that could enhance the current data set. Finally, a larger selection of candidate methods should be used for the sake of ensuring a thorough investigation is carried out.




